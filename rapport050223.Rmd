---
title: "Traitement des données avec HicImput en procedant à un morcellement des matrices scHic"
author: "SAMA Fousseni"
date: "`r Sys.Date()`"
output: word_document
---

```{r message=FALSE, warning=FALSE}
library("devtools")
## Warning: le package 'devtools' a été compilé avec la version R 4.1.3
## Le chargement a nécessité le package : usethis
## Warning: le package 'usethis' a été compilé avec la version R 4.1.3
# Install "HiCImpute" package from github.
# install_github("https://github.com/sl-lin/HiCImpute")
library("HiCImpute")

library("tidyverse")
library("FactoMineR")
library("factoextra")
library("ggpubr")
```


# Compte rendu de la rencontre du 03/02/2023
La rencotre du vendredi 03 février dernier avait pour obectif de trouver une piste de solution pour avancer dans le processus de traitement des données simulées. En effet, l'utilisation du programme HicImpute n'avait pas donné de résulats attendus et avec le programme de Higachi, le procéssus avait du mal à passer une dernière étape d'entrainenment (Etape 3) avant la visualisation des clusters.
A l'issue de la discussion, trois solutions avaient été proposées :
- Une première solution serait de morceller les matrices scHic avant de les soumettre au programme HicImpute : il sera question pour cette approche de solution de visualiser la concentration des contacts intrachromosomiques à l'aide d'un heatmap. Par la suite, les matrices doivent être coupées dans des régions où les contacts sont relativement concentrés avant de les soumettre chacune d'elles à HicImpute séparement. Ces différentes matrices doivent etre carrées et avoir la même diagonale que les matrices d'origine. De catte façon, les dimenssions des matrices seront reduises et par conséquent la complexité.
- Soumettre le programme de Higachi avec nos données scHic simulées comme input à une tache sur graham : le program de Higachi dans le processus d'exécution n'affichait pas d'erreur. Comme HicImput, ce dernier s'exécute sans aucune réponse et sans fin sur une durée de 4h sur graham. L'idée avec la tache est de soumettre le programme puis d'attendre jusqu'à ce qu'on ait une réponse. Lors de la souission, il sera question aussi de choisir une résolution intermédiaire entre 1Mb et 10Kb.
- Une troisième solution serait d'explorer l'article de Shichen Fan et ses collaborateurs (scHi-CSim: a flexible simulator that generates high-fidelity single-cell Hi-C data for benchmarking)

Traiter ces pistes de solution sera l'objet des travaux pour la suite et la prochaine rencontre pour discuter des résulats est fixée le mardi 07 Février 2023 à 14h.

# Visualisation des données scHic à l'aide d'un heatmap
La visualisation des données scHic sur un heatmap est une étape qui permettrait de desceller les régions qui ont une forte concentration de contacts.

```{r message=FALSE, warning=FALSE}
ncells = 2

for (i in seq_len(ncells)) {
  mat <- as.matrix(
    read.table(
      paste0("rdata/single_cell_hic_data/hic_mat_", sprintf("%03d", i), ".txt"), 
      quote="\"", 
      comment.char="", 
      stringsAsFactors = FALSE
    ) 
  )
  
  heatmap(mat)
}

```

Les heatmaps représentés ci-dessus nous donne une repartition visuelle de la concentration des contacts dans les matrices scHic. On constate que les contacts sont distribués de façon aléatoire. Il est difficile alors de sectionner une région spécifique des matrices. Par contre, on pourrait choisir de diviser les matrices en deux matrices carrées de 281 comme dimenssion.

# Résulats de l'imputation des données avec HicImpute


## Préparation de la matrices


```{r message=FALSE, warning=FALSE}
## Disposition des données dans une liste
# ncol = 562
# ncells = 250
# data_dim = ncol*(ncol-1)/2
# 
# scHic <- matrix(
#   0,
#   nrow = data_dim,
#   ncol = ncells,
#   byrow = TRUE
# )
# 
# for (i in seq_len(ncells)) {
#   mat <- as.matrix(
#     read.table(
#       paste0("rdata/single_cell_hic_data/hic_mat_", sprintf("%03d", i), ".txt"),
#       quote="\"",
#       comment.char="",
#       stringsAsFactors = FALSE
#     )
#   )
#   
#   scHic[, i] <- mat[upper.tri(mat)]
# }
# mat = NULL

```


## Imputation de la matrice 

```{r message=FALSE, warning=FALSE}
## HicImpute
# MCMCImpute_result=MCMCImpute(
#   scHiC=scHic,
#   bulk=apply(scHic,1,sum),
#   expected=NULL,
#   startval=c(100,100,10,8,10,0.1,900,0.2,0,replicate(dim(scHic)[2],8)),
#   n=ncol,
#   mc.cores = 5,
#   cutoff=0.5,
#   niter=10,
#   burnin=1
# )
# 
# save(MCMCImpute_result, file = "MCMCImpute_result.rda")

```


## Statistiques

```{r message=FALSE, warning=FALSE}
load("rdata/MCMCImpute_result.rda")
summary(MCMCImpute_result$SZ[upper.tri(MCMCImpute_result$SZ)])

# Nombre de 0 avant imputation
print(paste0("Le nombre de 0 avant imputation : ", sum(MCMCImpute_result$scHiC==0)))
# Nombre de 0 après imputation sans tenir compte des SZ
print(paste0("Le nombre de 0 imputés avant l'imputation des SZ : ", sum(MCMCImpute_result$scHiC==0) - sum(MCMCImpute_result$Impute_SZ==0)))
print(paste0("Le % de 0 imputés avant l'imputation des SZ : ", (sum(MCMCImpute_result$scHiC==0) - sum(MCMCImpute_result$Impute_SZ==0))/sum(MCMCImpute_result$scHiC==0)))

print(paste0("Le maximum des valeurs imputées : ", max(MCMCImpute_result$Impute_SZ)))
print(paste0("La valeur madiane parmi les valeurs imputées : ", median(MCMCImpute_result$Impute_SZ)))

# Après imputation des SZ
print(paste0("Le maximum des valeurs imputées après imputation complète : ", max(MCMCImpute_result$Impute_All)))
print(paste0("La valeur madiane parmi les valeurs imputées après imputation complète : ", median(MCMCImpute_result$Impute_All)))
```


# Clustering


## Importation des données depuis la grappe graham

```{r message=FALSE, warning=FALSE}
load("rdata/cluster_with_imp.rda")
load("rdata/cluster_without_imp.rda")
table(cluster_with_imp$cluster)
table(cluster_without_imp$cluster)

cluster_data <- as_tibble(
  data.frame(
    cluster_without_imp_clus = cluster_without_imp$cluster,
    cluster_without_imp_cells = names(cluster_without_imp$cluster),
    cluster_with_imp_clus = cluster_with_imp$cluster,
    cluster_with_imp_cells = names(cluster_with_imp$cluster)
  )
)
```


## Similarité entre les clustering des cellules avant et après imputation

```{r message=FALSE, warning=FALSE}
cluster_items_without_imp = unique(cluster_data$cluster_without_imp_clus)
cluster_items_with_imp = unique(cluster_data$cluster_with_imp_clus)

similarity_results = matrix(
  NA,
  nrow = length(cluster_items_without_imp), 
  ncol = 5
)

# Dans cette boucle, on parcour les clusters crées par HCPC et HicImput puis calcul la similarité entre les clusters. L'objectif est de voir dans les deux cas, quels sont les clusters qui sont similaires
for (i in seq(length(cluster_items_without_imp))) {
  
  # k parcours les différents clusters
  k = cluster_items_without_imp[i]
  # cellsi recupère les elements du cluster
  cellsi = cluster_data[cluster_data$cluster_without_imp_clus==k, ]$cluster_without_imp_cells
  # perc : est initialisé à 0 pour à chaque fois recupérer le % de similarité entre les clusters
  perc = 0
  for (j in cluster_items_with_imp) {
    
    cellsj = cluster_data[cluster_data$cluster_with_imp_clus==j, ]$cluster_with_imp_cells
    perc = length(intersect(cellsi, cellsj))/max(length(cellsi), length(cellsj))
    
    if(perc>0){
      cluster_to_keep = c(j, length(cellsj), perc)
    }
  }
  
  similarity_results[i, 1] <- i
  similarity_results[i, 2] <- length(cellsi)
  similarity_results[i, 3] <- cluster_to_keep[1]
  similarity_results[i, 4] <- cluster_to_keep[2]
  similarity_results[i, 5] <- cluster_to_keep[3]
  
  cluster_items_with_imp = cluster_items_with_imp[cluster_items_with_imp != cluster_to_keep[1]]
}

similarity_results

```



